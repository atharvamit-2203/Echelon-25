# -*- coding: utf-8 -*-
"""engagement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RFqVe1-2kyz-FrhDwC3qFozAjt8g4l-m
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load the dataset
print("ðŸ“Œ Loading dataset...")
df = pd.read_csv("/content/t.csv")

# Drop customer ID (not needed for analysis)
df.drop(columns=['customerID'], inplace=True, errors='ignore')

# Convert 'TotalCharges' to numeric and handle errors
print("ðŸ”„ Converting 'TotalCharges' to numeric...")
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Fill NaN values in 'TotalCharges' with median
df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)

# Handle missing values for other columns
df.fillna(method='ffill', inplace=True)

print("âœ… Missing values handled successfully!\n")

# Encode categorical variables
print("ðŸ”„ Encoding categorical variables...")
label_encoders = {}
categorical_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
                       'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                       'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',
                       'PaymentMethod', 'Churn']

for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

print("âœ… Categorical data encoded successfully!\n")

# Standardize only numerical columns
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

print("ðŸ”„ Standardizing numerical data...")
print("âœ… Numerical data scaled successfully!\n")

# Define engagement categories based on tenure & services used
print("ðŸ“Œ Categorizing customer engagement levels...")

def engagement_level(row):
    if row['tenure'] > 0.5 and row['OnlineSecurity'] == 1 and row['TechSupport'] == 1:
        return "Highly Engaged"
    elif row['tenure'] > 0.3 and row['StreamingTV'] == 1:
        return "Moderately Engaged"
    else:
        return "Low Engagement"

df['Engagement_Level'] = df.apply(engagement_level, axis=1)

print("âœ… Customers classified into engagement levels!\n")

# Display engagement distribution
print("ðŸ“Š Engagement Level Distribution:")
print(df['Engagement_Level'].value_counts(), "\n")

# Engagement Distribution Plot
sns.countplot(x="Engagement_Level", data=df, palette="coolwarm")
plt.title("Customer Engagement Levels")
plt.xlabel("Engagement Category")
plt.ylabel("Customer Count")
plt.show()

# **âœ… Exclude 'Engagement_Level' from correlation analysis**
corr_df = df.drop(columns=['Engagement_Level'], errors='ignore')

# Correlation Analysis - Understanding factors influencing engagement
print("ðŸ“ˆ Performing correlation analysis...\n")
plt.figure(figsize=(12,7))
sns.heatmap(corr_df.corr(), annot=True, cmap="coolwarm")
plt.title("Customer Engagement Correlation Analysis")
plt.show()

# Save results
df.to_csv("/content/customer_engagement_analysis.csv", index=False)
print("âœ… Engagement analysis saved successfully as 'customer_engagement_analysis.csv'!")

from transformers import pipeline

# Load GPT-2 Model Locally (Consider using GPT-Neo for better results)
generator = pipeline("text-generation", model="gpt2-medium")
  # Larger, better model

# Generate AI insights
def generate_local_ai_insights(data):
    prompt = f"""
    The following is an analysis of customer engagement levels:

    - Highly Engaged Customers: {data.get("Highly Engaged", 0)}
    - Moderately Engaged Customers: {data.get("Moderately Engaged", 0)}
    - Low Engagement Customers: {data.get("Low Engagement", 0)}

    Suggest **3 key strategies** to improve engagement and prevent customer churn.
    """

    response = generator(prompt, max_length=150, num_return_sequences=1)

    # Formatting response
    generated_text = response[0]['generated_text']
    insights = "\n".join(["âœ… " + line for line in generated_text.split(". ") if line.strip()])

    return insights

# ðŸ”¥ Get AI insights
ai_suggestions = generate_local_ai_insights(engagement_counts)

print("\nðŸ¤– **Locally Generated AI Insights:**")
print(ai_suggestions)